{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "#  Build a Deep Learning Model with Keras vs tf.estimator API", 
            "cell_type": "markdown", 
            "metadata": {
                "scrolled": true
            }
        }, 
        {
            "source": "Here we will use [MNIST](https://www.tensorflow.org/tutorials/) dataset to show how to make classification model using [Keras](https://keras.io/) and [tf.estimator API](https://www.tensorflow.org/api_docs/python/tf/estimator). Let's compare with those two models and at last I will dicuss about my opinion about those two.", 
            "cell_type": "markdown", 
            "metadata": {
                "scrolled": true
            }
        }, 
        {
            "source": "There 4 section included :\n- [Load dataset](#section1)\n- [Keras model](#section2)\n- [tf.estimator API](#section3)\n- [tf.estimator on large dataset](#section4)", 
            "cell_type": "markdown", 
            "metadata": {
                "scrolled": true
            }
        }, 
        {
            "source": "#### update tensorflow to the newest version", 
            "cell_type": "markdown", 
            "metadata": {
                "scrolled": true
            }
        }, 
        {
            "execution_count": 1, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Collecting tensorflow\n  Downloading https://files.pythonhosted.org/packages/b1/ad/48395de38c1e07bab85fc3bbec045e11ae49c02a4db0100463dd96031947/tensorflow-1.12.0-cp35-cp35m-manylinux1_x86_64.whl (83.1MB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 83.1MB 11kB/s  eta 0:00:01\n\u001b[?25hCollecting protobuf>=3.6.1 (from tensorflow)\n  Downloading https://files.pythonhosted.org/packages/bf/d4/db7296a1407cad69f043537ba1e05afab3646451a066ead7a314d8714388/protobuf-3.6.1-cp35-cp35m-manylinux1_x86_64.whl (1.1MB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.1MB 803kB/s eta 0:00:01\n\u001b[?25hCollecting absl-py>=0.1.6 (from tensorflow)\n  Downloading https://files.pythonhosted.org/packages/0c/63/f505d2d4c21db849cf80bad517f0065a30be6b006b0a5637f1b95584a305/absl-py-0.6.1.tar.gz (94kB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 102kB 9.6MB/s ta 0:00:01\n\u001b[?25hCollecting grpcio>=1.8.6 (from tensorflow)\n  Downloading https://files.pythonhosted.org/packages/d9/94/7c634ccc859169ceebca7140532b5267a0b0ed5583e1e624663997786895/grpcio-1.17.1-cp35-cp35m-manylinux1_x86_64.whl (10.1MB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10.1MB 97kB/s  eta 0:00:01:00:01\n\u001b[?25hRequirement not upgraded as not directly required: wheel>=0.26 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from tensorflow)\nCollecting keras-applications>=1.0.6 (from tensorflow)\n  Downloading https://files.pythonhosted.org/packages/3f/c4/2ff40221029f7098d58f8d7fb99b97e8100f3293f9856f0fb5834bef100b/Keras_Applications-1.0.6-py2.py3-none-any.whl (44kB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 9.6MB/s eta 0:00:01\n\u001b[?25hCollecting gast>=0.2.0 (from tensorflow)\n  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\nCollecting termcolor>=1.1.0 (from tensorflow)\n  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\nCollecting tensorboard<1.13.0,>=1.12.0 (from tensorflow)\n  Downloading https://files.pythonhosted.org/packages/07/53/8d32ce9471c18f8d99028b7cef2e5b39ea8765bd7ef250ca05b490880971/tensorboard-1.12.2-py3-none-any.whl (3.0MB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.1MB 317kB/s eta 0:00:01\n\u001b[?25hRequirement not upgraded as not directly required: six>=1.10.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from tensorflow)\nRequirement not upgraded as not directly required: numpy>=1.13.3 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from tensorflow)\nCollecting astor>=0.6.0 (from tensorflow)\n  Downloading https://files.pythonhosted.org/packages/35/6b/11530768cac581a12952a2aad00e1526b89d242d0b9f59534ef6e6a1752f/astor-0.7.1-py2.py3-none-any.whl\nCollecting keras-preprocessing>=1.0.5 (from tensorflow)\n  Downloading https://files.pythonhosted.org/packages/fc/94/74e0fa783d3fc07e41715973435dd051ca89c550881b3454233c39c73e69/Keras_Preprocessing-1.0.5-py2.py3-none-any.whl\nRequirement not upgraded as not directly required: setuptools in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from protobuf>=3.6.1->tensorflow)\nRequirement not upgraded as not directly required: h5py in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from keras-applications>=1.0.6->tensorflow)\nRequirement not upgraded as not directly required: markdown>=2.6.8 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow)\nRequirement not upgraded as not directly required: werkzeug>=0.11.10 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow)\nBuilding wheels for collected packages: absl-py, gast, termcolor\n  Running setup.py bdist_wheel for absl-py ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /home/dsxuser/.cache/pip/wheels/18/ea/5e/e36e1b8739e78cd2eba0a08fdc602c2b16a4b263912af8cb64\n  Running setup.py bdist_wheel for gast ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /home/dsxuser/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n  Running setup.py bdist_wheel for termcolor ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /home/dsxuser/.cache/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\nSuccessfully built absl-py gast termcolor\nInstalling collected packages: protobuf, absl-py, grpcio, keras-applications, gast, termcolor, tensorboard, astor, keras-preprocessing, tensorflow\n  Found existing installation: protobuf 3.4.1\n    Uninstalling protobuf-3.4.1:\n      Successfully uninstalled protobuf-3.4.1\n  Found existing installation: tensorflow 1.3.0\n    Uninstalling tensorflow-1.3.0:\n      Successfully uninstalled tensorflow-1.3.0\nSuccessfully installed absl-py-0.6.1 astor-0.7.1 gast-0.2.2 grpcio-1.17.1 keras-applications-1.0.6 keras-preprocessing-1.0.5 protobuf-3.6.1 tensorboard-1.12.2 tensorflow-1.12.0 termcolor-1.1.0\n"
                }
            ], 
            "source": "!pip install --upgrade tensorflow "
        }, 
        {
            "source": "#### import libraries", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 2, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stderr", 
                    "text": "Using TensorFlow backend.\n"
                }, 
                {
                    "execution_count": 2, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "'1.12.0'"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "import tensorflow as tf\nfrom keras.datasets import mnist\nfrom keras.layers import Flatten,Dense,Dropout\nfrom keras.models import Sequential\nimport numpy as np\nimport pandas as pd\ntf.__version__"
        }, 
        {
            "source": "#  <a name=\"section1\"></a> Load data", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n11493376/11490434 [==============================] - 0s 0us/step\n"
                }
            ], 
            "source": "(x_train, y_train),(x_test, y_test) = mnist.load_data()"
        }, 
        {
            "source": "####  inpect data", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Let's how many examples in training set and testing set. And each picture contains 28*28 pixels.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 4, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Traning set feature shape:  (60000, 28, 28) \n\nTraning set label shape:  (60000,) \n\nTesting set feature shape:  (60000, 28, 28) \n\nTesting set feature shape:  (60000,) \n\n"
                }
            ], 
            "source": "print (\"Traning set feature shape: \", x_train.shape,\"\\n\")\nprint (\"Traning set label shape: \", y_train.shape,\"\\n\")\nprint (\"Testing set feature shape: \", x_train.shape,\"\\n\")\nprint (\"Testing set feature shape: \", y_train.shape,\"\\n\")"
        }, 
        {
            "execution_count": 5, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 5, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "y_train[0:10]"
        }, 
        {
            "source": "So features contian 28*28 pixals  0-255 grey scale,\n\nlabel is list of number 0-9. Now let's normalize feature between 0 and 1", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 6, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# noramlized features from 0-1  \nx_train, x_test = x_train / 255.0, x_test / 255.0"
        }, 
        {
            "execution_count": 7, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# define input_size\ndim=28"
        }, 
        {
            "source": "# <a name=\"section2\"></a> Keras model\nlets run a baby example using keras", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "*Sequential* is defined in [Keras.layers](https://keras.io/layers/about-keras-layers/) module, once we define **\"model=Sequential()\"**. We then could used model.add() function to add any layers to define the model complexity.  \nOne another thing need to notice: the first layer need a **input_shape** arugement to define the input tensor shape\nHow easy it is?!   \nThe last line of code **\"model.summary()\"** return a table of parameters of the model you just build.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 8, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nflatten_1 (Flatten)          (None, 784)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 512)               401920    \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 512)               0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 10)                5130      \n=================================================================\nTotal params: 407,050\nTrainable params: 407,050\nNon-trainable params: 0\n_________________________________________________________________\n"
                }
            ], 
            "source": "model = Sequential()\nmodel.add(Flatten(input_shape=(dim,dim,)))\nmodel.add(Dense(512, activation=tf.nn.relu))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(10, activation=tf.nn.softmax))\nmodel.summary()"
        }, 
        {
            "source": "Once the model has been defined, we need [compile](https://keras.io/models/model/#compile) the model and pass arguments like *optimizer*, *loss* and *metrics*.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 9, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "model.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])"
        }, 
        {
            "source": "Then use **fit** function and pass **features tensors(or numpy arrays)** and **labels**. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 10, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Epoch 1/5\n60000/60000 [==============================] - 93s 2ms/step - loss: 0.2188 - acc: 0.9351\nEpoch 2/5\n60000/60000 [==============================] - 85s 1ms/step - loss: 0.0975 - acc: 0.9707\nEpoch 3/5\n60000/60000 [==============================] - 102s 2ms/step - loss: 0.0675 - acc: 0.9788\nEpoch 4/5\n60000/60000 [==============================] - 109s 2ms/step - loss: 0.0539 - acc: 0.9825\nEpoch 5/5\n60000/60000 [==============================] - 95s 2ms/step - loss: 0.0440 - acc: 0.9858\n"
                }, 
                {
                    "execution_count": 10, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "<keras.callbacks.History at 0x7f7966ae4898>"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "model.fit(x_train, y_train, batch_size= 32, epochs=5)"
        }, 
        {
            "execution_count": 11, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "10000/10000 [==============================] - 3s 307us/step\n"
                }, 
                {
                    "execution_count": 11, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[0.068943760562455284, 0.9788]"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "model.evaluate(x_test, y_test)"
        }, 
        {
            "source": "It is returning the loss and metrics we difined before.  \nI got accuracy: 0.9788 on my eval set on 5 epochs from scrach.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# <a name=\"section3\"></a> tf.estimator API", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### DNNClassifier estimator API \nthe following code in this section is modified from a [blog](https://codeburst.io/use-tensorflow-dnnclassifier-estimator-to-classify-mnist-dataset-a7222bf9f940) by *Macro Lanaro*.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "To build estimator api, it needs to define a *\"input function\"* including *\"feature columns\"* and *\"classifier\"*. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 49, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Define the training inputs\ntrain_input_fn = tf.estimator.inputs.numpy_input_fn(\n    x={\"x\": x_train},\n    y=y_train.astype(np.int32),\n    num_epochs=None,\n    batch_size=50,\n    shuffle=True\n)"
        }, 
        {
            "source": "**tf.feature_columns** is a powerful tool to define different type of features. Use *feature_columns* function, we can easily define different type of columns like **bucketized_column, categorical_column_with_hash_bucket, categorical_column_with_identity**, which means if the dataset needs feature engineering before training, this would be a good choice.  (See other *feature_columns* [here](https://www.tensorflow.org/api_docs/python/tf/feature_column))", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 45, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Specify feature\nfeature_columns = [tf.feature_column.numeric_column(\"x\", shape=[28, 28])]"
        }, 
        {
            "source": "There are some pre-defined models(Regressor and Classifier) in estimator API. I will list some and give some short explanations here:  \n[BaselineClassifier](https://www.tensorflow.org/api_docs/python/tf/estimator/BaselineClassifier): This classifier ignores feature values and will learn to predict the average value of each label. For single-label problems, this will predict the probability distribution of the classes as seen in the labels. For multi-label problems, this will predict the fraction of examples that are positive for each class. (**It basically do nothing.**)  \n[BoostedTreesClassifier](https://www.tensorflow.org/api_docs/python/tf/estimator/BoostedTreesClassifier): A Classifier for Tensorflow Boosted Trees models. (I have not used TFBT yet, but If you want to know more about TFBT , you can find some sources [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/boosted_trees) and [there](http://ecmlpkdd2017.ijs.si/papers/paperID705.pdf))  \n[DNNClassifier](https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier): A classifier for TensorFlow DNN models. (**It is easy to define a simple DNN model by using like \"hidden_units=[1024, 512, 256]\"**)  \n[DNNLinearCombinedClassifier](https://www.tensorflow.org/api_docs/python/tf/estimator/DNNLinearCombinedClassifier): An estimator for TensorFlow Linear and DNN joined classification models. (**This is more costomized model. If some of columns you want to use Linear and some of the columns DNN, this model would be great. \"linear_feature_columns=[...], dnn_feature_columns=[..]\"**)  \n[LinearClassifier](https://www.tensorflow.org/api_docs/python/tf/estimator/LinearClassifier):Train a linear model to classify instances into one of multiple possible classes. When number of possible classes is 2, this is binary classification. (**It is a Logisticregression with binary or Softmax layer with multiclasses, simple and easy**)\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 53, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "INFO:tensorflow:Using default config.\nINFO:tensorflow:Using config: {'_save_summary_steps': 100, '_save_checkpoints_steps': None, '_global_id_in_cluster': 0, '_task_id': 0, '_task_type': 'worker', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f780c5a7278>, '_num_worker_replicas': 1, '_protocol': None, '_eval_distribute': None, '_master': '', '_is_chief': True, '_tf_random_seed': None, '_session_config': allow_soft_placement: true\ngraph_options {\n  rewrite_options {\n    meta_optimizer_iterations: ONE\n  }\n}\n, '_num_ps_replicas': 0, '_keep_checkpoint_every_n_hours': 10000, '_save_checkpoints_secs': 600, '_train_distribute': None, '_experimental_distribute': None, '_model_dir': './tmp/mnist_model', '_log_step_count_steps': 100, '_device_fn': None, '_keep_checkpoint_max': 5, '_service': None, '_evaluation_master': ''}\n"
                }
            ], 
            "source": "# Build 2 layer DNN classifier\nclassifier = tf.estimator.DNNClassifier(\n    feature_columns=feature_columns,\n    hidden_units=[256],\n    optimizer=tf.train.AdamOptimizer(1e-4),\n    n_classes=10,\n    dropout=0.2,\n    model_dir=\"./tmp/mnist_model\"\n)"
        }, 
        {
            "execution_count": 54, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "INFO:tensorflow:Calling model_fn.\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Create CheckpointSaverHook.\nINFO:tensorflow:Graph was finalized.\nINFO:tensorflow:Running local_init_op.\nINFO:tensorflow:Done running local_init_op.\nINFO:tensorflow:Saving checkpoints for 0 into ./tmp/mnist_model/model.ckpt.\nINFO:tensorflow:loss = 121.802, step = 1\nINFO:tensorflow:global_step/sec: 61.4317\nINFO:tensorflow:loss = 68.5095, step = 101 (1.702 sec)\nINFO:tensorflow:global_step/sec: 62.6354\nINFO:tensorflow:loss = 49.4702, step = 201 (1.514 sec)\nINFO:tensorflow:global_step/sec: 70.9626\nINFO:tensorflow:loss = 37.7748, step = 301 (1.405 sec)\nINFO:tensorflow:global_step/sec: 67.2957\nINFO:tensorflow:loss = 31.825, step = 401 (1.485 sec)\nINFO:tensorflow:global_step/sec: 52.5849\nINFO:tensorflow:loss = 28.4717, step = 501 (1.902 sec)\nINFO:tensorflow:global_step/sec: 62.6429\nINFO:tensorflow:loss = 21.2868, step = 601 (1.596 sec)\nINFO:tensorflow:global_step/sec: 62.5158\nINFO:tensorflow:loss = 33.1921, step = 701 (1.599 sec)\nINFO:tensorflow:global_step/sec: 62.3679\nINFO:tensorflow:loss = 17.8847, step = 801 (1.604 sec)\nINFO:tensorflow:global_step/sec: 71.6003\nINFO:tensorflow:loss = 27.3183, step = 901 (1.397 sec)\nINFO:tensorflow:global_step/sec: 62.0781\nINFO:tensorflow:loss = 18.1688, step = 1001 (1.611 sec)\nINFO:tensorflow:global_step/sec: 66.5458\nINFO:tensorflow:loss = 19.8525, step = 1101 (1.503 sec)\nINFO:tensorflow:global_step/sec: 62.9073\nINFO:tensorflow:loss = 19.2064, step = 1201 (1.590 sec)\nINFO:tensorflow:global_step/sec: 58.811\nINFO:tensorflow:loss = 37.9015, step = 1301 (1.700 sec)\nINFO:tensorflow:global_step/sec: 62.5433\nINFO:tensorflow:loss = 15.6147, step = 1401 (1.599 sec)\nINFO:tensorflow:global_step/sec: 62.4846\nINFO:tensorflow:loss = 16.1482, step = 1501 (1.601 sec)\nINFO:tensorflow:global_step/sec: 66.0724\nINFO:tensorflow:loss = 13.6277, step = 1601 (1.593 sec)\nINFO:tensorflow:global_step/sec: 59.1684\nINFO:tensorflow:loss = 20.6206, step = 1701 (1.610 sec)\nINFO:tensorflow:global_step/sec: 71.0037\nINFO:tensorflow:loss = 18.4882, step = 1801 (1.409 sec)\nINFO:tensorflow:global_step/sec: 66.718\nINFO:tensorflow:loss = 18.6254, step = 1901 (1.499 sec)\nINFO:tensorflow:global_step/sec: 71.2125\nINFO:tensorflow:loss = 24.3903, step = 2001 (1.404 sec)\nINFO:tensorflow:global_step/sec: 62.8458\nINFO:tensorflow:loss = 13.7387, step = 2101 (1.591 sec)\nINFO:tensorflow:global_step/sec: 62.2757\nINFO:tensorflow:loss = 10.8889, step = 2201 (1.606 sec)\nINFO:tensorflow:global_step/sec: 55.6268\nINFO:tensorflow:loss = 14.1414, step = 2301 (1.798 sec)\nINFO:tensorflow:global_step/sec: 62.5372\nINFO:tensorflow:loss = 17.201, step = 2401 (1.599 sec)\nINFO:tensorflow:global_step/sec: 62.8682\nINFO:tensorflow:loss = 21.4618, step = 2501 (1.591 sec)\nINFO:tensorflow:global_step/sec: 66.0703\nINFO:tensorflow:loss = 22.8158, step = 2601 (1.513 sec)\nINFO:tensorflow:global_step/sec: 59.2054\nINFO:tensorflow:loss = 15.5993, step = 2701 (1.689 sec)\nINFO:tensorflow:global_step/sec: 62.5741\nINFO:tensorflow:loss = 8.24551, step = 2801 (1.600 sec)\nINFO:tensorflow:global_step/sec: 66.7778\nINFO:tensorflow:loss = 9.84984, step = 2901 (1.496 sec)\nINFO:tensorflow:global_step/sec: 76.5505\nINFO:tensorflow:loss = 11.4913, step = 3001 (1.306 sec)\nINFO:tensorflow:global_step/sec: 71.0544\nINFO:tensorflow:loss = 12.5438, step = 3101 (1.408 sec)\nINFO:tensorflow:global_step/sec: 62.8107\nINFO:tensorflow:loss = 14.3298, step = 3201 (1.595 sec)\nINFO:tensorflow:global_step/sec: 66.9391\nINFO:tensorflow:loss = 18.0646, step = 3301 (1.491 sec)\nINFO:tensorflow:global_step/sec: 54.992\nINFO:tensorflow:loss = 8.15245, step = 3401 (1.819 sec)\nINFO:tensorflow:global_step/sec: 67.029\nINFO:tensorflow:loss = 10.1809, step = 3501 (1.492 sec)\nINFO:tensorflow:global_step/sec: 67.2156\nINFO:tensorflow:loss = 5.40207, step = 3601 (1.488 sec)\nINFO:tensorflow:global_step/sec: 65.9882\nINFO:tensorflow:loss = 14.5899, step = 3701 (1.515 sec)\nINFO:tensorflow:global_step/sec: 62.8134\nINFO:tensorflow:loss = 13.0695, step = 3801 (1.595 sec)\nINFO:tensorflow:global_step/sec: 16.9465\nINFO:tensorflow:loss = 16.3003, step = 3901 (5.898 sec)\nINFO:tensorflow:global_step/sec: 62.7308\nINFO:tensorflow:loss = 16.1697, step = 4001 (1.594 sec)\nINFO:tensorflow:global_step/sec: 70.9951\nINFO:tensorflow:loss = 5.55165, step = 4101 (1.410 sec)\nINFO:tensorflow:global_step/sec: 66.228\nINFO:tensorflow:loss = 7.96833, step = 4201 (1.587 sec)\nINFO:tensorflow:global_step/sec: 56.1865\nINFO:tensorflow:loss = 8.52587, step = 4301 (1.701 sec)\nINFO:tensorflow:global_step/sec: 66.4251\nINFO:tensorflow:loss = 6.85529, step = 4401 (1.505 sec)\nINFO:tensorflow:global_step/sec: 58.7\nINFO:tensorflow:loss = 10.1988, step = 4501 (1.704 sec)\nINFO:tensorflow:global_step/sec: 58.9096\nINFO:tensorflow:loss = 8.05029, step = 4601 (1.698 sec)\nINFO:tensorflow:global_step/sec: 66.6605\nINFO:tensorflow:loss = 14.5497, step = 4701 (1.500 sec)\nINFO:tensorflow:global_step/sec: 66.7875\nINFO:tensorflow:loss = 21.4583, step = 4801 (1.497 sec)\nINFO:tensorflow:global_step/sec: 71.282\nINFO:tensorflow:loss = 9.96876, step = 4901 (1.405 sec)\nINFO:tensorflow:global_step/sec: 71.2905\nINFO:tensorflow:loss = 18.527, step = 5001 (1.403 sec)\nINFO:tensorflow:global_step/sec: 66.8843\nINFO:tensorflow:loss = 10.306, step = 5101 (1.493 sec)\nINFO:tensorflow:global_step/sec: 66.8142\nINFO:tensorflow:loss = 6.91562, step = 5201 (1.497 sec)\nINFO:tensorflow:global_step/sec: 55.2372\nINFO:tensorflow:loss = 7.09249, step = 5301 (1.813 sec)\nINFO:tensorflow:global_step/sec: 62.6079\nINFO:tensorflow:loss = 6.97725, step = 5401 (1.595 sec)\nINFO:tensorflow:global_step/sec: 62.4493\nINFO:tensorflow:loss = 10.8742, step = 5501 (1.689 sec)\nINFO:tensorflow:global_step/sec: 66.6577\nINFO:tensorflow:loss = 3.34514, step = 5601 (1.415 sec)\nINFO:tensorflow:global_step/sec: 77.119\nINFO:tensorflow:loss = 11.7359, step = 5701 (1.295 sec)\nINFO:tensorflow:global_step/sec: 62.2129\nINFO:tensorflow:loss = 7.89293, step = 5801 (1.693 sec)\nINFO:tensorflow:global_step/sec: 62.8196\nINFO:tensorflow:loss = 5.07294, step = 5901 (1.505 sec)\nINFO:tensorflow:global_step/sec: 58.9503\nINFO:tensorflow:loss = 8.46785, step = 6001 (1.696 sec)\nINFO:tensorflow:global_step/sec: 71.2018\nINFO:tensorflow:loss = 5.75749, step = 6101 (1.404 sec)\nINFO:tensorflow:global_step/sec: 62.1246\nINFO:tensorflow:loss = 10.8719, step = 6201 (1.610 sec)\nINFO:tensorflow:global_step/sec: 66.9153\nINFO:tensorflow:loss = 7.60483, step = 6301 (1.584 sec)\nINFO:tensorflow:global_step/sec: 62.7724\nINFO:tensorflow:loss = 5.19591, step = 6401 (1.503 sec)\nINFO:tensorflow:global_step/sec: 55.6651\nINFO:tensorflow:loss = 7.07051, step = 6501 (1.798 sec)\nINFO:tensorflow:global_step/sec: 62.2735\nINFO:tensorflow:loss = 14.4383, step = 6601 (1.605 sec)\nINFO:tensorflow:global_step/sec: 58.8694\nINFO:tensorflow:loss = 8.78651, step = 6701 (1.699 sec)\nINFO:tensorflow:global_step/sec: 66.1878\nINFO:tensorflow:loss = 7.1201, step = 6801 (1.510 sec)\nINFO:tensorflow:global_step/sec: 71.8224\nINFO:tensorflow:loss = 5.48161, step = 6901 (1.392 sec)\nINFO:tensorflow:global_step/sec: 66.7632\nINFO:tensorflow:loss = 11.9077, step = 7001 (1.498 sec)\nINFO:tensorflow:global_step/sec: 66.604\nINFO:tensorflow:loss = 4.50428, step = 7101 (1.501 sec)\nINFO:tensorflow:global_step/sec: 62.6609\nINFO:tensorflow:loss = 12.5488, step = 7201 (1.599 sec)\nINFO:tensorflow:global_step/sec: 66.0236\nINFO:tensorflow:loss = 14.973, step = 7301 (1.591 sec)\nINFO:tensorflow:global_step/sec: 66.8932\nINFO:tensorflow:loss = 5.47284, step = 7401 (1.415 sec)\nINFO:tensorflow:global_step/sec: 62.6644\nINFO:tensorflow:loss = 14.1632, step = 7501 (1.596 sec)\nINFO:tensorflow:global_step/sec: 66.49\nINFO:tensorflow:loss = 10.5252, step = 7601 (1.504 sec)\nINFO:tensorflow:global_step/sec: 70.3663\nINFO:tensorflow:loss = 4.26116, step = 7701 (1.421 sec)\nINFO:tensorflow:global_step/sec: 59.8709\nINFO:tensorflow:loss = 3.39557, step = 7801 (1.671 sec)\nINFO:tensorflow:global_step/sec: 55.65\nINFO:tensorflow:loss = 2.8155, step = 7901 (1.796 sec)\nINFO:tensorflow:global_step/sec: 58.8193\nINFO:tensorflow:loss = 2.47339, step = 8001 (1.703 sec)\nINFO:tensorflow:global_step/sec: 62.389\nINFO:tensorflow:loss = 7.56428, step = 8101 (1.601 sec)\nINFO:tensorflow:global_step/sec: 69.6628\nINFO:tensorflow:loss = 4.73977, step = 8201 (1.435 sec)\nINFO:tensorflow:global_step/sec: 48.4644\nINFO:tensorflow:loss = 7.85998, step = 8301 (2.065 sec)\nINFO:tensorflow:global_step/sec: 62.0602\nINFO:tensorflow:loss = 6.39131, step = 8401 (1.609 sec)\nINFO:tensorflow:global_step/sec: 71.5862\nINFO:tensorflow:loss = 14.1189, step = 8501 (1.397 sec)\nINFO:tensorflow:global_step/sec: 52.8087\nINFO:tensorflow:loss = 12.1342, step = 8601 (1.894 sec)\nINFO:tensorflow:global_step/sec: 62.7249\nINFO:tensorflow:loss = 4.8915, step = 8701 (1.596 sec)\nINFO:tensorflow:global_step/sec: 62.2642\nINFO:tensorflow:loss = 11.0851, step = 8801 (1.604 sec)\nINFO:tensorflow:global_step/sec: 66.7522\nINFO:tensorflow:loss = 4.0989, step = 8901 (1.498 sec)\nINFO:tensorflow:global_step/sec: 66.1793\nINFO:tensorflow:loss = 1.84748, step = 9001 (1.511 sec)\nINFO:tensorflow:global_step/sec: 66.8259\nINFO:tensorflow:loss = 3.20477, step = 9101 (1.496 sec)\nINFO:tensorflow:global_step/sec: 59.0497\nINFO:tensorflow:loss = 9.07242, step = 9201 (1.695 sec)\nINFO:tensorflow:global_step/sec: 52.7599\nINFO:tensorflow:loss = 2.82035, step = 9301 (1.894 sec)\nINFO:tensorflow:global_step/sec: 61.8954\nINFO:tensorflow:loss = 3.23482, step = 9401 (1.615 sec)\nINFO:tensorflow:global_step/sec: 59.2671\nINFO:tensorflow:loss = 2.7215, step = 9501 (1.688 sec)\nINFO:tensorflow:global_step/sec: 62.0381\nINFO:tensorflow:loss = 6.26391, step = 9601 (1.611 sec)\nINFO:tensorflow:global_step/sec: 52.917\nINFO:tensorflow:loss = 10.2159, step = 9701 (1.890 sec)\nINFO:tensorflow:global_step/sec: 55.5259\nINFO:tensorflow:loss = 9.42682, step = 9801 (1.801 sec)\nINFO:tensorflow:global_step/sec: 66.5029\nINFO:tensorflow:loss = 12.5752, step = 9901 (1.503 sec)\nINFO:tensorflow:global_step/sec: 66.1434\nINFO:tensorflow:loss = 10.2458, step = 10001 (1.512 sec)\nINFO:tensorflow:global_step/sec: 62.9436\nINFO:tensorflow:loss = 2.65545, step = 10101 (1.589 sec)\nINFO:tensorflow:global_step/sec: 71.5901\nINFO:tensorflow:loss = 2.37675, step = 10201 (1.397 sec)\nINFO:tensorflow:global_step/sec: 66.2973\nINFO:tensorflow:loss = 4.77555, step = 10301 (1.508 sec)\nINFO:tensorflow:global_step/sec: 62.1294\nINFO:tensorflow:loss = 8.18309, step = 10401 (1.609 sec)\nINFO:tensorflow:global_step/sec: 59.6997\nINFO:tensorflow:loss = 9.54808, step = 10501 (1.675 sec)\nINFO:tensorflow:global_step/sec: 55.1247\nINFO:tensorflow:loss = 1.80476, step = 10601 (1.814 sec)\nINFO:tensorflow:global_step/sec: 66.7733\nINFO:tensorflow:loss = 6.50839, step = 10701 (1.497 sec)\nINFO:tensorflow:global_step/sec: 66.6505\nINFO:tensorflow:loss = 4.24331, step = 10801 (1.500 sec)\nINFO:tensorflow:global_step/sec: 55.879\nINFO:tensorflow:loss = 13.358, step = 10901 (1.789 sec)\nINFO:tensorflow:global_step/sec: 62.0916\nINFO:tensorflow:loss = 7.45668, step = 11001 (1.611 sec)\nINFO:tensorflow:global_step/sec: 52.9397\nINFO:tensorflow:loss = 6.84107, step = 11101 (1.889 sec)\nINFO:tensorflow:global_step/sec: 62.5447\nINFO:tensorflow:loss = 4.74012, step = 11201 (1.599 sec)\nINFO:tensorflow:global_step/sec: 65.9246\nINFO:tensorflow:loss = 4.4362, step = 11301 (1.599 sec)\nINFO:tensorflow:global_step/sec: 66.5267\nINFO:tensorflow:loss = 1.79998, step = 11401 (1.500 sec)\nINFO:tensorflow:global_step/sec: 59.206\nINFO:tensorflow:loss = 12.167, step = 11501 (1.610 sec)\nINFO:tensorflow:global_step/sec: 66.991\nINFO:tensorflow:loss = 2.93986, step = 11601 (1.493 sec)\nINFO:tensorflow:global_step/sec: 70.7423\nINFO:tensorflow:loss = 6.62872, step = 11701 (1.413 sec)\nINFO:tensorflow:global_step/sec: 59.3226\nINFO:tensorflow:loss = 9.38785, step = 11801 (1.685 sec)\nINFO:tensorflow:global_step/sec: 58.86\nINFO:tensorflow:loss = 5.04938, step = 11901 (1.899 sec)\nINFO:tensorflow:global_step/sec: 55.2269\nINFO:tensorflow:loss = 8.66158, step = 12001 (1.611 sec)\nINFO:tensorflow:global_step/sec: 66.5097\nINFO:tensorflow:loss = 6.62202, step = 12101 (1.503 sec)\nINFO:tensorflow:global_step/sec: 62.7002\nINFO:tensorflow:loss = 6.12852, step = 12201 (1.595 sec)\nINFO:tensorflow:global_step/sec: 62.593\nINFO:tensorflow:loss = 3.37703, step = 12301 (1.598 sec)\nINFO:tensorflow:global_step/sec: 58.6526\nINFO:tensorflow:loss = 2.0968, step = 12401 (1.792 sec)\nINFO:tensorflow:global_step/sec: 62.9158\nINFO:tensorflow:loss = 7.35824, step = 12501 (1.502 sec)\nINFO:tensorflow:global_step/sec: 71.021\nINFO:tensorflow:loss = 4.83631, step = 12601 (1.408 sec)\nINFO:tensorflow:global_step/sec: 58.9876\nINFO:tensorflow:loss = 1.85133, step = 12701 (1.695 sec)\nINFO:tensorflow:global_step/sec: 52.5123\nINFO:tensorflow:loss = 2.11502, step = 12801 (1.904 sec)\nINFO:tensorflow:global_step/sec: 62.7736\nINFO:tensorflow:loss = 2.72864, step = 12901 (1.593 sec)\nINFO:tensorflow:global_step/sec: 58.451\nINFO:tensorflow:loss = 3.97841, step = 13001 (1.711 sec)\nINFO:tensorflow:global_step/sec: 62.7049\nINFO:tensorflow:loss = 11.2965, step = 13101 (1.595 sec)\nINFO:tensorflow:global_step/sec: 58.7487\nINFO:tensorflow:loss = 11.6658, step = 13201 (1.702 sec)\nINFO:tensorflow:global_step/sec: 52.8437\nINFO:tensorflow:loss = 4.72883, step = 13301 (1.892 sec)\nINFO:tensorflow:global_step/sec: 52.5559\nINFO:tensorflow:loss = 4.83573, step = 13401 (1.908 sec)\nINFO:tensorflow:global_step/sec: 62.2096\nINFO:tensorflow:loss = 8.99661, step = 13501 (1.692 sec)\nINFO:tensorflow:global_step/sec: 58.879\nINFO:tensorflow:loss = 1.46018, step = 13601 (1.609 sec)\nINFO:tensorflow:global_step/sec: 62.8967\nINFO:tensorflow:loss = 1.22616, step = 13701 (1.590 sec)\nINFO:tensorflow:global_step/sec: 43.4568\nINFO:tensorflow:loss = 1.85968, step = 13801 (2.302 sec)\nINFO:tensorflow:global_step/sec: 66.7098\nINFO:tensorflow:loss = 2.79163, step = 13901 (1.499 sec)\nINFO:tensorflow:global_step/sec: 62.2024\nINFO:tensorflow:loss = 8.51551, step = 14001 (1.608 sec)\nINFO:tensorflow:global_step/sec: 62.6752\nINFO:tensorflow:loss = 3.44269, step = 14101 (1.598 sec)\nINFO:tensorflow:global_step/sec: 62.6373\nINFO:tensorflow:loss = 5.5648, step = 14201 (1.594 sec)\nINFO:tensorflow:global_step/sec: 58.2751\nINFO:tensorflow:loss = 5.48194, step = 14301 (1.716 sec)\nINFO:tensorflow:global_step/sec: 58.923\nINFO:tensorflow:loss = 3.21667, step = 14401 (1.698 sec)\nINFO:tensorflow:global_step/sec: 71.4846\nINFO:tensorflow:loss = 3.45093, step = 14501 (1.401 sec)\nINFO:tensorflow:global_step/sec: 71.4924\nINFO:tensorflow:loss = 8.94037, step = 14601 (1.396 sec)\nINFO:tensorflow:global_step/sec: 65.6546\nINFO:tensorflow:loss = 4.23431, step = 14701 (1.523 sec)\nINFO:tensorflow:global_step/sec: 56.3113\nINFO:tensorflow:loss = 5.0881, step = 14801 (1.776 sec)\nINFO:tensorflow:global_step/sec: 55.4434\nINFO:tensorflow:loss = 7.08764, step = 14901 (1.888 sec)\nINFO:tensorflow:global_step/sec: 62.3519\nINFO:tensorflow:loss = 7.98236, step = 15001 (1.519 sec)\nINFO:tensorflow:global_step/sec: 67.0226\nINFO:tensorflow:loss = 3.29371, step = 15101 (1.492 sec)\nINFO:tensorflow:global_step/sec: 62.7911\nINFO:tensorflow:loss = 3.65796, step = 15201 (1.593 sec)\nINFO:tensorflow:global_step/sec: 70.8896\nINFO:tensorflow:loss = 6.20197, step = 15301 (1.410 sec)\nINFO:tensorflow:global_step/sec: 62.6965\nINFO:tensorflow:loss = 1.49016, step = 15401 (1.595 sec)\nINFO:tensorflow:global_step/sec: 62.4318\nINFO:tensorflow:loss = 1.65005, step = 15501 (1.601 sec)\nINFO:tensorflow:global_step/sec: 62.8338\nINFO:tensorflow:loss = 3.2754, step = 15601 (1.592 sec)\nINFO:tensorflow:global_step/sec: 66.3715\nINFO:tensorflow:loss = 1.44377, step = 15701 (1.507 sec)\nINFO:tensorflow:global_step/sec: 62.1721\nINFO:tensorflow:loss = 8.97553, step = 15801 (1.609 sec)\nINFO:tensorflow:global_step/sec: 66.8652\nINFO:tensorflow:loss = 7.00178, step = 15901 (1.496 sec)\nINFO:tensorflow:global_step/sec: 67.1619\nINFO:tensorflow:loss = 2.45056, step = 16001 (1.491 sec)\nINFO:tensorflow:global_step/sec: 70.9643\nINFO:tensorflow:loss = 2.03676, step = 16101 (1.409 sec)\nINFO:tensorflow:global_step/sec: 67.0895\nINFO:tensorflow:loss = 3.01222, step = 16201 (1.490 sec)\nINFO:tensorflow:global_step/sec: 58.289\nINFO:tensorflow:loss = 4.2512, step = 16301 (1.796 sec)\nINFO:tensorflow:global_step/sec: 56.0495\nINFO:tensorflow:loss = 7.4468, step = 16401 (1.702 sec)\nINFO:tensorflow:global_step/sec: 62.455\nINFO:tensorflow:loss = 7.53804, step = 16501 (1.601 sec)\nINFO:tensorflow:global_step/sec: 58.7013\nINFO:tensorflow:loss = 1.24997, step = 16601 (1.704 sec)\nINFO:tensorflow:global_step/sec: 62.3069\nINFO:tensorflow:loss = 3.44681, step = 16701 (1.605 sec)\nINFO:tensorflow:global_step/sec: 62.3789\nINFO:tensorflow:loss = 3.37909, step = 16801 (1.603 sec)\nINFO:tensorflow:global_step/sec: 67.2195\nINFO:tensorflow:loss = 3.15107, step = 16901 (1.487 sec)\nINFO:tensorflow:global_step/sec: 66.47\nINFO:tensorflow:loss = 1.51554, step = 17001 (1.505 sec)\nINFO:tensorflow:global_step/sec: 66.3423\nINFO:tensorflow:loss = 2.61759, step = 17101 (1.507 sec)\nINFO:tensorflow:global_step/sec: 71.6806\nINFO:tensorflow:loss = 7.31032, step = 17201 (1.395 sec)\nINFO:tensorflow:global_step/sec: 58.5228\nINFO:tensorflow:loss = 1.44817, step = 17301 (1.709 sec)\nINFO:tensorflow:global_step/sec: 62.8317\nINFO:tensorflow:loss = 6.4842, step = 17401 (1.591 sec)\nINFO:tensorflow:global_step/sec: 70.9055\nINFO:tensorflow:loss = 1.23821, step = 17501 (1.491 sec)\nINFO:tensorflow:global_step/sec: 59.4282\nINFO:tensorflow:loss = 1.71916, step = 17601 (1.602 sec)\nINFO:tensorflow:global_step/sec: 62.3814\nINFO:tensorflow:loss = 1.12171, step = 17701 (1.603 sec)\nINFO:tensorflow:global_step/sec: 45.1835\nINFO:tensorflow:loss = 6.56466, step = 17801 (2.213 sec)\nINFO:tensorflow:global_step/sec: 59.3089\nINFO:tensorflow:loss = 1.78898, step = 17901 (1.686 sec)\nINFO:tensorflow:global_step/sec: 71.4822\nINFO:tensorflow:loss = 3.41242, step = 18001 (1.399 sec)\nINFO:tensorflow:global_step/sec: 62.5135\nINFO:tensorflow:loss = 3.67466, step = 18101 (1.605 sec)\nINFO:tensorflow:global_step/sec: 62.1726\nINFO:tensorflow:loss = 6.63775, step = 18201 (1.604 sec)\nINFO:tensorflow:global_step/sec: 66.2871\nINFO:tensorflow:loss = 0.73981, step = 18301 (1.508 sec)\nINFO:tensorflow:global_step/sec: 50.0412\nINFO:tensorflow:loss = 6.49247, step = 18401 (2.000 sec)\nINFO:tensorflow:global_step/sec: 66.9418\nINFO:tensorflow:loss = 0.60717, step = 18501 (1.492 sec)\nINFO:tensorflow:global_step/sec: 62.6991\nINFO:tensorflow:loss = 1.11131, step = 18601 (1.595 sec)\nINFO:tensorflow:global_step/sec: 62.3879\nINFO:tensorflow:loss = 0.907614, step = 18701 (1.603 sec)\nINFO:tensorflow:global_step/sec: 62.4047\nINFO:tensorflow:loss = 2.1291, step = 18801 (1.603 sec)\nINFO:tensorflow:global_step/sec: 70.7539\nINFO:tensorflow:loss = 4.11129, step = 18901 (1.413 sec)\nINFO:tensorflow:global_step/sec: 63.3963\nINFO:tensorflow:loss = 5.44846, step = 19001 (1.577 sec)\nINFO:tensorflow:global_step/sec: 49.9234\nINFO:tensorflow:loss = 5.62491, step = 19101 (2.008 sec)\nINFO:tensorflow:global_step/sec: 52.7318\nINFO:tensorflow:loss = 7.74991, step = 19201 (1.893 sec)\nINFO:tensorflow:global_step/sec: 66.4908\nINFO:tensorflow:loss = 4.44105, step = 19301 (1.503 sec)\nINFO:tensorflow:global_step/sec: 66.7745\nINFO:tensorflow:loss = 2.3852, step = 19401 (1.500 sec)\nINFO:tensorflow:global_step/sec: 62.3077\nINFO:tensorflow:loss = 1.24064, step = 19501 (1.602 sec)\nINFO:tensorflow:global_step/sec: 66.9137\nINFO:tensorflow:loss = 3.32384, step = 19601 (1.494 sec)\nINFO:tensorflow:global_step/sec: 66.5352\nINFO:tensorflow:loss = 1.18686, step = 19701 (1.503 sec)\nINFO:tensorflow:global_step/sec: 71.2248\nINFO:tensorflow:loss = 1.4706, step = 19801 (1.404 sec)\nINFO:tensorflow:global_step/sec: 58.7617\nINFO:tensorflow:loss = 2.12143, step = 19901 (1.702 sec)\nINFO:tensorflow:global_step/sec: 52.2433\nINFO:tensorflow:loss = 5.88507, step = 20001 (1.914 sec)\nINFO:tensorflow:global_step/sec: 67.2775\nINFO:tensorflow:loss = 0.728642, step = 20101 (1.487 sec)\nINFO:tensorflow:global_step/sec: 62.5287\nINFO:tensorflow:loss = 4.13613, step = 20201 (1.599 sec)\nINFO:tensorflow:global_step/sec: 62.2724\nINFO:tensorflow:loss = 6.14378, step = 20301 (1.606 sec)\nINFO:tensorflow:global_step/sec: 59.1515\nINFO:tensorflow:loss = 1.61584, step = 20401 (1.690 sec)\nINFO:tensorflow:global_step/sec: 70.4658\nINFO:tensorflow:loss = 2.9576, step = 20501 (1.419 sec)\nINFO:tensorflow:global_step/sec: 67.0172\nINFO:tensorflow:loss = 4.18499, step = 20601 (1.573 sec)\nINFO:tensorflow:global_step/sec: 58.8131\nINFO:tensorflow:loss = 2.83805, step = 20701 (1.620 sec)\nINFO:tensorflow:global_step/sec: 67.0521\nINFO:tensorflow:loss = 5.63633, step = 20801 (1.491 sec)\nINFO:tensorflow:global_step/sec: 66.6048\nINFO:tensorflow:loss = 3.61582, step = 20901 (1.501 sec)\nINFO:tensorflow:global_step/sec: 66.5481\nINFO:tensorflow:loss = 2.29087, step = 21001 (1.502 sec)\nINFO:tensorflow:global_step/sec: 62.6564\nINFO:tensorflow:loss = 2.8032, step = 21101 (1.596 sec)\nINFO:tensorflow:global_step/sec: 66.6184\nINFO:tensorflow:loss = 1.69553, step = 21201 (1.501 sec)\nINFO:tensorflow:global_step/sec: 71.6782\nINFO:tensorflow:loss = 2.76058, step = 21301 (1.395 sec)\nINFO:tensorflow:global_step/sec: 71.3994\nINFO:tensorflow:loss = 3.61501, step = 21401 (1.401 sec)\nINFO:tensorflow:global_step/sec: 52.4695\nINFO:tensorflow:loss = 2.40454, step = 21501 (1.906 sec)\nINFO:tensorflow:global_step/sec: 67.0995\nINFO:tensorflow:loss = 4.98355, step = 21601 (1.491 sec)\nINFO:tensorflow:global_step/sec: 66.3162\nINFO:tensorflow:loss = 13.1601, step = 21701 (1.508 sec)\nINFO:tensorflow:global_step/sec: 66.8712\nINFO:tensorflow:loss = 3.45241, step = 21801 (1.495 sec)\nINFO:tensorflow:global_step/sec: 66.8277\nINFO:tensorflow:loss = 5.79841, step = 21901 (1.496 sec)\nINFO:tensorflow:global_step/sec: 62.1822\nINFO:tensorflow:loss = 1.91356, step = 22001 (1.609 sec)\nINFO:tensorflow:global_step/sec: 59.1608\nINFO:tensorflow:loss = 1.22457, step = 22101 (1.691 sec)\nINFO:tensorflow:global_step/sec: 62.4456\nINFO:tensorflow:loss = 2.17203, step = 22201 (1.601 sec)\nINFO:tensorflow:global_step/sec: 66.1798\nINFO:tensorflow:loss = 2.53251, step = 22301 (1.511 sec)\nINFO:tensorflow:global_step/sec: 66.6269\nINFO:tensorflow:loss = 6.91364, step = 22401 (1.501 sec)\nINFO:tensorflow:global_step/sec: 59.1021\nINFO:tensorflow:loss = 2.07916, step = 22501 (1.692 sec)\nINFO:tensorflow:global_step/sec: 62.6792\nINFO:tensorflow:loss = 3.86921, step = 22601 (1.596 sec)\nINFO:tensorflow:global_step/sec: 55.3434\nINFO:tensorflow:loss = 5.96153, step = 22701 (1.807 sec)\nINFO:tensorflow:global_step/sec: 66.7819\nINFO:tensorflow:loss = 5.21667, step = 22801 (1.497 sec)\nINFO:tensorflow:global_step/sec: 62.7447\nINFO:tensorflow:loss = 2.17067, step = 22901 (1.597 sec)\nINFO:tensorflow:global_step/sec: 52.3343\nINFO:tensorflow:loss = 4.34011, step = 23001 (1.908 sec)\nINFO:tensorflow:global_step/sec: 66.8098\nINFO:tensorflow:loss = 4.96399, step = 23101 (1.497 sec)\nINFO:tensorflow:global_step/sec: 62.7562\nINFO:tensorflow:loss = 4.86404, step = 23201 (1.593 sec)\nINFO:tensorflow:global_step/sec: 66.0625\nINFO:tensorflow:loss = 1.67714, step = 23301 (1.597 sec)\nINFO:tensorflow:global_step/sec: 59.2796\nINFO:tensorflow:loss = 4.59056, step = 23401 (1.604 sec)\nINFO:tensorflow:global_step/sec: 66.0573\nINFO:tensorflow:loss = 2.6657, step = 23501 (1.514 sec)\nINFO:tensorflow:global_step/sec: 71.1182\nINFO:tensorflow:loss = 5.5271, step = 23601 (1.406 sec)\nINFO:tensorflow:global_step/sec: 56.2021\nINFO:tensorflow:loss = 3.77428, step = 23701 (1.779 sec)\nINFO:tensorflow:global_step/sec: 66.7144\nINFO:tensorflow:loss = 5.46438, step = 23801 (1.499 sec)\nINFO:tensorflow:global_step/sec: 70.6711\nINFO:tensorflow:loss = 8.34928, step = 23901 (1.417 sec)\nINFO:tensorflow:global_step/sec: 55.9896\nINFO:tensorflow:loss = 6.51198, step = 24001 (1.785 sec)\nINFO:tensorflow:global_step/sec: 66.24\nINFO:tensorflow:loss = 6.46996, step = 24101 (1.508 sec)\nINFO:tensorflow:global_step/sec: 66.412\nINFO:tensorflow:loss = 4.98276, step = 24201 (1.510 sec)\nINFO:tensorflow:global_step/sec: 66.939\nINFO:tensorflow:loss = 5.06682, step = 24301 (1.490 sec)\nINFO:tensorflow:global_step/sec: 66.5723\nINFO:tensorflow:loss = 0.884624, step = 24401 (1.502 sec)\nINFO:tensorflow:global_step/sec: 66.3561\nINFO:tensorflow:loss = 3.55032, step = 24501 (1.507 sec)\nINFO:tensorflow:global_step/sec: 62.322\nINFO:tensorflow:loss = 1.41036, step = 24601 (1.614 sec)\nINFO:tensorflow:global_step/sec: 63.2661\nINFO:tensorflow:loss = 3.69165, step = 24701 (1.571 sec)\nINFO:tensorflow:global_step/sec: 70.9835\nINFO:tensorflow:loss = 0.625495, step = 24801 (1.409 sec)\nINFO:tensorflow:global_step/sec: 62.4408\nINFO:tensorflow:loss = 2.14022, step = 24901 (1.602 sec)\nINFO:tensorflow:global_step/sec: 66.8896\nINFO:tensorflow:loss = 1.82245, step = 25001 (1.495 sec)\nINFO:tensorflow:global_step/sec: 62.736\nINFO:tensorflow:loss = 1.94663, step = 25101 (1.594 sec)\nINFO:tensorflow:global_step/sec: 70.1815\nINFO:tensorflow:loss = 3.87769, step = 25201 (1.427 sec)\nINFO:tensorflow:global_step/sec: 62.9132\nINFO:tensorflow:loss = 5.28026, step = 25301 (1.587 sec)\nINFO:tensorflow:global_step/sec: 71.6041\nINFO:tensorflow:loss = 1.80573, step = 25401 (1.482 sec)\nINFO:tensorflow:global_step/sec: 55.583\nINFO:tensorflow:loss = 1.6548, step = 25501 (1.714 sec)\nINFO:tensorflow:global_step/sec: 62.5768\nINFO:tensorflow:loss = 4.10314, step = 25601 (1.599 sec)\nINFO:tensorflow:global_step/sec: 66.6209\nINFO:tensorflow:loss = 2.42826, step = 25701 (1.501 sec)\nINFO:tensorflow:global_step/sec: 62.7834\nINFO:tensorflow:loss = 4.4398, step = 25801 (1.593 sec)\nINFO:tensorflow:global_step/sec: 62.3267\nINFO:tensorflow:loss = 3.41601, step = 25901 (1.604 sec)\nINFO:tensorflow:global_step/sec: 66.7089\nINFO:tensorflow:loss = 2.13032, step = 26001 (1.499 sec)\nINFO:tensorflow:global_step/sec: 50.229\nINFO:tensorflow:loss = 2.95025, step = 26101 (1.991 sec)\nINFO:tensorflow:global_step/sec: 66.4419\nINFO:tensorflow:loss = 1.38499, step = 26201 (1.505 sec)\nINFO:tensorflow:global_step/sec: 70.7909\nINFO:tensorflow:loss = 7.4967, step = 26301 (1.413 sec)\nINFO:tensorflow:global_step/sec: 67.1525\nINFO:tensorflow:loss = 1.75697, step = 26401 (1.489 sec)\nINFO:tensorflow:global_step/sec: 52.3003\nINFO:tensorflow:loss = 0.849341, step = 26501 (1.912 sec)\nINFO:tensorflow:global_step/sec: 56.1507\nINFO:tensorflow:loss = 6.49441, step = 26601 (1.781 sec)\nINFO:tensorflow:global_step/sec: 82.1044\nINFO:tensorflow:loss = 1.43898, step = 26701 (1.299 sec)\nINFO:tensorflow:global_step/sec: 62.9056\nINFO:tensorflow:loss = 2.32675, step = 26801 (1.508 sec)\nINFO:tensorflow:global_step/sec: 62.2052\nINFO:tensorflow:loss = 0.363385, step = 26901 (1.608 sec)\nINFO:tensorflow:global_step/sec: 62.7751\nINFO:tensorflow:loss = 5.17893, step = 27001 (1.593 sec)\nINFO:tensorflow:global_step/sec: 62.2846\nINFO:tensorflow:loss = 3.20398, step = 27101 (1.606 sec)\nINFO:tensorflow:global_step/sec: 62.646\nINFO:tensorflow:loss = 2.44348, step = 27201 (1.596 sec)\nINFO:tensorflow:global_step/sec: 62.7066\nINFO:tensorflow:loss = 4.14998, step = 27301 (1.594 sec)\nINFO:tensorflow:global_step/sec: 66.2015\nINFO:tensorflow:loss = 1.43873, step = 27401 (1.511 sec)\nINFO:tensorflow:global_step/sec: 66.6004\nINFO:tensorflow:loss = 4.35953, step = 27501 (1.501 sec)\nINFO:tensorflow:global_step/sec: 67.1299\nINFO:tensorflow:loss = 4.98034, step = 27601 (1.489 sec)\nINFO:tensorflow:global_step/sec: 66.8949\nINFO:tensorflow:loss = 0.457042, step = 27701 (1.496 sec)\nINFO:tensorflow:global_step/sec: 62.5492\nINFO:tensorflow:loss = 2.2295, step = 27801 (1.601 sec)\nINFO:tensorflow:global_step/sec: 62.119\nINFO:tensorflow:loss = 1.33891, step = 27901 (1.607 sec)\nINFO:tensorflow:global_step/sec: 55.5105\nINFO:tensorflow:loss = 0.864555, step = 28001 (1.802 sec)\nINFO:tensorflow:global_step/sec: 58.6545\nINFO:tensorflow:loss = 0.84185, step = 28101 (1.788 sec)\nINFO:tensorflow:global_step/sec: 55.6414\nINFO:tensorflow:loss = 1.14384, step = 28201 (1.717 sec)\nINFO:tensorflow:global_step/sec: 62.9771\nINFO:tensorflow:loss = 3.7685, step = 28301 (1.585 sec)\nINFO:tensorflow:global_step/sec: 70.7512\nINFO:tensorflow:loss = 2.43438, step = 28401 (1.414 sec)\nINFO:tensorflow:global_step/sec: 63.0453\nINFO:tensorflow:loss = 1.55619, step = 28501 (1.586 sec)\nINFO:tensorflow:global_step/sec: 66.075\nINFO:tensorflow:loss = 1.6467, step = 28601 (1.513 sec)\nINFO:tensorflow:global_step/sec: 55.6513\nINFO:tensorflow:loss = 1.44003, step = 28701 (1.797 sec)\nINFO:tensorflow:global_step/sec: 58.6934\nINFO:tensorflow:loss = 0.898293, step = 28801 (1.704 sec)\nINFO:tensorflow:global_step/sec: 52.9944\nINFO:tensorflow:loss = 0.452232, step = 28901 (1.887 sec)\nINFO:tensorflow:global_step/sec: 62.3262\nINFO:tensorflow:loss = 0.909782, step = 29001 (1.604 sec)\nINFO:tensorflow:global_step/sec: 66.6463\nINFO:tensorflow:loss = 4.25549, step = 29101 (1.501 sec)\nINFO:tensorflow:global_step/sec: 62.6922\nINFO:tensorflow:loss = 1.46972, step = 29201 (1.598 sec)\nINFO:tensorflow:global_step/sec: 66.5259\nINFO:tensorflow:loss = 0.90285, step = 29301 (1.502 sec)\nINFO:tensorflow:global_step/sec: 70.934\nINFO:tensorflow:loss = 1.59605, step = 29401 (1.493 sec)\nINFO:tensorflow:global_step/sec: 55.5452\nINFO:tensorflow:loss = 2.24467, step = 29501 (1.718 sec)\nINFO:tensorflow:global_step/sec: 62.767\nINFO:tensorflow:loss = 2.64073, step = 29601 (1.591 sec)\nINFO:tensorflow:global_step/sec: 58.6681\nINFO:tensorflow:loss = 1.61933, step = 29701 (1.708 sec)\nINFO:tensorflow:global_step/sec: 62.565\nINFO:tensorflow:loss = 3.28505, step = 29801 (1.595 sec)\nINFO:tensorflow:global_step/sec: 62.9197\nINFO:tensorflow:loss = 0.954338, step = 29901 (1.592 sec)\nINFO:tensorflow:Saving checkpoints for 30000 into ./tmp/mnist_model/model.ckpt.\nINFO:tensorflow:Loss for final step: 2.16675.\n"
                }, 
                {
                    "execution_count": 54, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "<tensorflow.python.estimator.canned.dnn.DNNClassifier at 0x7f780c5a71d0>"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "import shutil\nshutil.rmtree(\"./tmp/mnist_model\", ignore_errors = True)\nclassifier.train(input_fn=train_input_fn, steps=30000)"
        }, 
        {
            "source": "In order to estimate the accuracy of the model, we need to define another \"input function\"", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 56, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Define the test inputs\ntest_input_fn = tf.estimator.inputs.numpy_input_fn(\n    x={\"x\": x_test},\n    y=y_test.astype(np.int32),\n    num_epochs=1,\n    shuffle=False\n)\n"
        }, 
        {
            "execution_count": 57, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "INFO:tensorflow:Calling model_fn.\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Starting evaluation at 2019-01-15-01:08:47\nINFO:tensorflow:Graph was finalized.\nINFO:tensorflow:Restoring parameters from ./tmp/mnist_model/model.ckpt-30000\nINFO:tensorflow:Running local_init_op.\nINFO:tensorflow:Done running local_init_op.\nINFO:tensorflow:Finished evaluation at 2019-01-15-01:08:50\nINFO:tensorflow:Saving dict for global step 30000: accuracy = 0.9809, average_loss = 0.0659267, global_step = 30000, loss = 8.34515\nINFO:tensorflow:Saving 'checkpoint_path' summary for global step 30000: ./tmp/mnist_model/model.ckpt-30000\n\nTest Accuracy: 98.089999%\n\n"
                }
            ], 
            "source": "# Evaluate accuracy\naccuracy_score = classifier.evaluate(input_fn=test_input_fn)[\"accuracy\"]\nprint(\"\\nTest Accuracy: {0:f}%\\n\".format(accuracy_score*100))"
        }, 
        {
            "source": "I got accuracy about 98.089999% on eval set.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# <a name=\"section4\"></a> Create TensorFlow model using TensorFlow's Estimator API\ntf.estimator", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "In previous sections, we learned how to **Kereas** and **estimator** API to build a simple model. The dataset is downloaded and saved in numpy arrays. When you need to hundle a large dataset like 10GB. The memory can handle it in pandas or numpy. TF API need to read data directly from csv file lines batch by batch. They are good for distributed learning as well.  \nThis section is modifed from the google cloud platform repository [here](https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/courses/machine_learning/deepdive/06_structured/3_tensorflow_dnn.ipynb). ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "**add and save files for training and testing.csv**", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 13, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "pd.DataFrame(x_train.reshape(-1,dim*dim)).join(pd.DataFrame(y_train,columns=['label'])).to_csv('train.csv',index=False,header=False)"
        }, 
        {
            "source": "I seem it will take about 1 minute to compile those files and save in pandas.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 14, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "pd.DataFrame(x_test.reshape(-1,dim*dim)).join(pd.DataFrame(y_test,columns=['label'])).to_csv('test.csv',index=False,header=False)"
        }, 
        {
            "source": "let us see how the df look like", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 15, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "temp=pd.read_csv('train.csv',header=None)"
        }, 
        {
            "execution_count": 12, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 12, 
                    "metadata": {}, 
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>775</th>\n      <th>776</th>\n      <th>777</th>\n      <th>778</th>\n      <th>779</th>\n      <th>780</th>\n      <th>781</th>\n      <th>782</th>\n      <th>783</th>\n      <th>784</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>9</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows \u00d7 785 columns</p>\n</div>", 
                        "text/plain": "   0    1    2    3    4    5    6    7    8    9   ...   775  776  777  778  \\\n0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0  0.0   \n1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0  0.0   \n2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0  0.0   \n3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0  0.0   \n4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0  0.0   \n\n   779  780  781  782  783  784  \n0  0.0  0.0  0.0  0.0  0.0    5  \n1  0.0  0.0  0.0  0.0  0.0    0  \n2  0.0  0.0  0.0  0.0  0.0    4  \n3  0.0  0.0  0.0  0.0  0.0    1  \n4  0.0  0.0  0.0  0.0  0.0    9  \n\n[5 rows x 785 columns]"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "temp.head()"
        }, 
        {
            "execution_count": 55, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 55, 
                    "metadata": {}, 
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>775</th>\n      <th>776</th>\n      <th>777</th>\n      <th>778</th>\n      <th>779</th>\n      <th>780</th>\n      <th>781</th>\n      <th>782</th>\n      <th>783</th>\n      <th>784</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows \u00d7 785 columns</p>\n</div>", 
                        "text/plain": "   0    1    2    3    4    5    6    7    8    9   ...   775  776  777  778  \\\n0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0  0.0   \n1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0  0.0   \n2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0  0.0   \n3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0  0.0   \n4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0  0.0   \n\n   779  780  781  782  783  784  \n0  0.0  0.0  0.0  0.0  0.0    7  \n1  0.0  0.0  0.0  0.0  0.0    2  \n2  0.0  0.0  0.0  0.0  0.0    1  \n3  0.0  0.0  0.0  0.0  0.0    0  \n4  0.0  0.0  0.0  0.0  0.0    4  \n\n[5 rows x 785 columns]"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "temp2=pd.read_csv('test.csv',header=None)\ntemp2.head()"
        }, 
        {
            "source": "Columns 0-783 are pixels for the image; and column 784 is the label", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### Define read_dataset\nWe have to create a input_function to read data from files batch by batch and pack them into a dict with \"key\" equal to column names and \"value\" equal to values as input data source.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 58, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#define columns here\nCSV_COLUMNS = [\"pixel\"+str(i) for i in range (dim*dim)]+['digit']\nLABEL_COLUMN = 'digit'\nDEFAULTS = [[0.0] for i in range(dim*dim)]+[[0]] # [[0],['NA'],[0]]\n\ndef read_dataset(filename, mode, batch_size=32):\n    def _input_fn():\n        def decode_csv(value_colum):\n            columns = tf.decode_csv(value_colum,record_defaults = DEFAULTS)\n            features= dict(zip(CSV_COLUMNS,columns))\n            label = features.pop (LABEL_COLUMN)\n            return features, label\n        \n        # Create list of files that match pattern\n        file_list =  tf.gfile.Glob(filename)\n        \n        # Create dataset from file list\n        dataset = tf.data.TextLineDataset(filename).map(decode_csv) # Transform each elem by applying decode_csv fn\n        if mode == tf.estimator.ModeKeys.TRAIN:\n            num_epochs = None   # indefinitely\n            dataset = dataset.shuffle(buffer_size =10 *batch_size)\n        else:\n            num_epochs = 1   # end-of-input after this\n        dataset = dataset.repeat(num_epochs).batch(batch_size)\n\n        return dataset.make_one_shot_iterator().get_next()\n    return _input_fn"
        }, 
        {
            "source": "#### Define a feature columns\nBecause we have grey scale values for the pixels we can just use **numeric_column** for those columns.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 59, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 59, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "''"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "# Define feature columns  - not including label\ndef get_cols():\n  # Define column types\n  return [tf.feature_column.numeric_column('pixel'+str(i)) for i in range(dim*dim)]\n"
        }, 
        {
            "source": "#### Define a severing function\nby defining the severing input, we could use it to evaluate the model. The serving functing is being used as **exporters** in **EvalSpec**. Basically, it tell what is the format when evaluate the model, it should be same as the traning format. Export your model to work with JSON dictionaries.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 73, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Create serving input function to be able to serve predictions later using provided inputs\ndef serving_input_fn():\n    csv_row = tf.placeholder(shape=[None], dtype=tf.string)\n    columns = tf.decode_csv(csv_row,record_defaults = DEFAULTS[:-1])\n    features= dict(zip(CSV_COLUMNS,columns))\n    #feature_placeholders = dict(zip(['pixel'+str(i) for i in range(dim*dim)],  \n    #                                [tf.placeholder(tf.float32, [None]) for i in range(dim*dim)]))\n    \n    #features = {key: tf.expand_dims(tensor,-1) for key,tensor in feature_placeholders.items()}\n\n    return tf.estimator.export.ServingInputReceiver(features, {'csv_row': csv_row})#feature_placeholders)"
        }, 
        {
            "source": "This function we define a **train_and_evaluate** function to complile all parts together.  \nIt include three main section: *estimator*, *train_spec*, and *eval_spec*. \n*estimator* : here we just use a simple DNNClassifier here, we can also build Convolutional models as well later.\n*train_spec* : pass input_fn as we defined before, and traning steps.\n*eval_spec*: define the evaluation freqencies", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 75, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def train_and_evaluate(output_dir):\n    EVAL_INTERVAL = 300  #save checkpoint every 300s\n    TRAIN_STEPS = 300\n    run_config = tf.estimator.RunConfig(save_checkpoints_secs = EVAL_INTERVAL)\n    \n    estimator = tf.estimator.DNNClassifier (model_dir=output_dir, \n                                         feature_columns = get_cols(),hidden_units=[32],\n                                            dropout=0.2,\n                                            n_classes=10,\n                                        config = run_config)\n\n    exporter = tf.estimator.LatestExporter('exporter', serving_input_fn)\n\n    train_spec = tf.estimator.TrainSpec(input_fn =  read_dataset('train.csv', mode = tf.contrib.learn.ModeKeys.TRAIN),\n                                        max_steps= TRAIN_STEPS)\n\n    \n    eval_spec = tf.estimator.EvalSpec(input_fn= read_dataset('test.csv', mode = tf.contrib.learn.ModeKeys.EVAL), \n                                        steps = None,\n                                        start_delay_secs = 60, # start evaluating after N seconds\n                                        throttle_secs= EVAL_INTERVAL,  # evaluate every N seconds\n                                        exporters = exporter)\n\n    tf.estimator.train_and_evaluate(estimator,train_spec,eval_spec)"
        }, 
        {
            "execution_count": 76, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "INFO:tensorflow:Using config: {'_save_summary_steps': 100, '_save_checkpoints_steps': None, '_global_id_in_cluster': 0, '_task_id': 0, '_task_type': 'worker', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f780c2ae198>, '_num_worker_replicas': 1, '_protocol': None, '_eval_distribute': None, '_master': '', '_is_chief': True, '_tf_random_seed': None, '_session_config': allow_soft_placement: true\ngraph_options {\n  rewrite_options {\n    meta_optimizer_iterations: ONE\n  }\n}\n, '_num_ps_replicas': 0, '_keep_checkpoint_every_n_hours': 10000, '_save_checkpoints_secs': 300, '_train_distribute': None, '_experimental_distribute': None, '_model_dir': 'mnist', '_log_step_count_steps': 100, '_device_fn': None, '_keep_checkpoint_max': 5, '_service': None, '_evaluation_master': ''}\nINFO:tensorflow:Not using Distribute Coordinator.\nINFO:tensorflow:Running training and evaluation locally (non-distributed).\nINFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 300.\nINFO:tensorflow:Calling model_fn.\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Create CheckpointSaverHook.\nINFO:tensorflow:Graph was finalized.\nINFO:tensorflow:Running local_init_op.\nINFO:tensorflow:Done running local_init_op.\nINFO:tensorflow:Saving checkpoints for 0 into mnist/model.ckpt.\nINFO:tensorflow:loss = 75.8332, step = 1\nINFO:tensorflow:global_step/sec: 8.96992\nINFO:tensorflow:loss = 7.4517, step = 101 (11.153 sec)\nINFO:tensorflow:global_step/sec: 12.7129\nINFO:tensorflow:loss = 11.7008, step = 201 (7.863 sec)\nINFO:tensorflow:Saving checkpoints for 300 into mnist/model.ckpt.\nINFO:tensorflow:Calling model_fn.\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Starting evaluation at 2019-01-15-02:18:11\nINFO:tensorflow:Graph was finalized.\nINFO:tensorflow:Restoring parameters from mnist/model.ckpt-300\nINFO:tensorflow:Running local_init_op.\nINFO:tensorflow:Done running local_init_op.\nINFO:tensorflow:Finished evaluation at 2019-01-15-02:18:37\nINFO:tensorflow:Saving dict for global step 300: accuracy = 0.9088, average_loss = 0.335611, global_step = 300, loss = 10.7224\nINFO:tensorflow:Saving 'checkpoint_path' summary for global step 300: mnist/model.ckpt-300\nINFO:tensorflow:Calling model_fn.\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Signatures INCLUDED in export for Train: None\nINFO:tensorflow:Signatures INCLUDED in export for Regress: None\nINFO:tensorflow:Signatures INCLUDED in export for Eval: None\nINFO:tensorflow:Signatures INCLUDED in export for Classify: ['classification', 'serving_default']\nINFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict']\nINFO:tensorflow:Restoring parameters from mnist/model.ckpt-300\nINFO:tensorflow:Assets added to graph.\nINFO:tensorflow:No assets to write.\nINFO:tensorflow:SavedModel written to: mnist/export/exporter/temp-b'1547518717'/saved_model.pb\nINFO:tensorflow:Loss for final step: 8.53443.\n"
                }
            ], 
            "source": "import shutil\nshutil.rmtree('mnist', ignore_errors = True)\ntrain_and_evaluate('mnist')"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.5", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}